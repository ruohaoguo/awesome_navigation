# Embodied Navigation

---
<font size=5><center><b> Contents </b> </center></font>
- [Navigation-LLM](#navigation-llm)
- [Navigation-OV](#navigation-ov)
---

## Navigation-LLM
|  Title  |   Date  |   Venue   |   Code   |
|:--------|:--------:|:--------:|:--------:|
| [**LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action**](https://arxiv.org/pdf/2207.04429) | 2022.07 | PMLR-23 | [GitHub](https://github.com/blazejosinski/lm_nav) |
| [**Visual Language Maps for Robot Navigation**](https://arxiv.org/pdf/2210.05714) | 2022.10 | ICRA-23 | [GitHub](https://github.com/vlmaps/vlmaps) |
| [**SQA3D: Situated Question Answering in 3D Scenes**](https://arxiv.org/pdf/2210.07474) | 2022.10 | ICLR-23 | [GitHub](https://github.com/SilongYong/SQA3D) |
| [**CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation**](https://arxiv.org/pdf/2211.16649) | 2022.11 | CoRL-W-22 |  |
| [**ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation**](https://arxiv.org/pdf/2301.13166) | 2023.01 | ICML-23 |  |
| [**Can an Embodied Agent Find Your "Cat-shaped Mug"? LLM-Guided Exploration for Zero-Shot Object Navigation**](https://arxiv.org/pdf/2303.03480) | 2023.03 | IRAL-24 | [GitHub](https://github.com/vdorbala/LGX) |
| [**L3MVN: Leveraging Large Language Models for Visual Target Navigation**](https://arxiv.org/pdf/2304.05501) | 2023.04 | IROS-23 | [GitHub](https://github.com/ybgdgh/L3MVN) |
| [**NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models**](https://arxiv.org/pdf/2305.16986) | 2023.05 | AAAI-24 | [GitHub](https://github.com/GengzeZhou/NavGPT) |
| [**CorNav: Autonomous Agent with Self-Corrected Planning for Zero-Shot Vision-and-Language Navigation**](https://arxiv.org/pdf/2306.10322) | 2023.06 | ArXiv-23 | [GitHub](https://github.com/mligg23/CorNav-Site) |
| [**VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View**](https://arxiv.org/pdf/2307.06082) | 2023.07 | AAAI-24 | [GitHub](https://github.com/raphael-sch/VELMA) |
| [**A2 Nav: Action-Aware Zero-Shot Robot Navigation by Exploiting Vision-and-Language Ability of Foundation Models**](https://arxiv.org/pdf/2308.07997) | 2023.08 | NeurIPS-W-23 |  |
| [**March in Chat: Interactive Prompting for Remote Embodied Referring Expression**](https://arxiv.org/pdf/2308.10141) | 2023.08 | ICCV-23 | [GitHub](https://github.com/YanyuanQiao/MiC) |
| [**Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill**](https://arxiv.org/pdf/2309.10309) | 2023.09 | ICRA-24 | [GitHub](https://github.com/wzcai99/Pixel-Navigator) |
| [**Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions**](https://arxiv.org/pdf/2309.11382) | 2023.09 | ICRA-24 | [GitHub](https://github.com/LYX0501/DiscussNav) |
| [**Optimal Scene Graph Planning with Large Language Model Guidance**](https://arxiv.org/pdf/2309.09182) | 2023.09 | ICRA-24 | [GitHub](https://github.com/ExistentialRobotics/LLM-Scene-Graph-LTL-Planning) |
| [**SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments**](https://arxiv.org/pdf/2309.04077) | 2023.09 | ICAPS-24 | [GitHub](https://github.com/arajv/SayNav) |
| [**Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation**](https://arxiv.org/pdf/2309.08138) | 2023.09 | NeurIPS-23 | [GitHub](https://github.com/whcpumpkin/Demand-driven-navigation) |
| [**Reasoning about the Unseen for Efficient Outdoor Object Navigation**](https://arxiv.org/pdf/2309.10103) | 2023.09 | ArXiv-23 | [GitHub](https://github.com/quantingxie/ReasonedExplorer) |
| [**Multimodal Large Language Model for Visual Navigation**](https://arxiv.org/pdf/2310.08669) | 2023.10 | ArXiv-23 |  |
| [**LangNav: Language as a Perceptual Representation for Navigation**](https://arxiv.org/pdf/2310.07889) | 2023.10 | ArXiv-23 |  |
| [**Vision and Language Navigation in the Real World via Online Visual Language Mapping**](https://arxiv.org/pdf/2310.10822) | 2023.10 | ArXiv-23 |  |
| [**Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning**](https://arxiv.org/pdf/2310.10103) | 2023.10 | CoRL-23 | [GitHub](https://github.com/Michael-Equi/lfg-nav) |
| [**ThinkBot: Embodied Instruction Following with Thought Chain Reasoning**](https://arxiv.org/pdf/2312.07062) | 2023.12 | Submit to ICLR-25 |  |
| [**Towards Learning a Generalist Model for Embodied Navigation**](https://arxiv.org/pdf/2312.02010) | 2023.12 | CVPR-24 | [GitHub](https://github.com/zd11024/NaviLLM) |
| [**VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model**](https://arxiv.org/pdf/2401.02695) | 2024.01 | ICML-24 |  |
| [**OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models**](https://arxiv.org/pdf/2402.10670) | 2024.02 | NAACL-24 | [GitHub](https://github.com/yxKryptonite/OpenFMNav) |
| [**Verifiably Following Complex Robot Instructions with Foundation Models**](https://arxiv.org/pdf/2402.11498) | 2024.02 | ArXiv-24 |  |
| [**DOZE: A Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments**](https://arxiv.org/pdf/2402.19007) | 2024.02 | IRAL-24 | [GitHub](https://github.com/JiMa25/DOZE-Dataset) |
| [**TriHelper: Zero-Shot Object Navigation with Dynamic Assistance**](https://arxiv.org/pdf/2403.15223) | 2024.03 | IROS-24 |  |
| [**Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation**](https://arxiv.org/pdf/2403.14163) | 2024.03 | Submit to AEI-24 | [GitHub](https://github.com/yxKryptonite/OpenFMNav) |
| [**Correctable Landmark Discovery via Large Models for Vision-Language Navigation**](https://arxiv.org/pdf/2405.18721) | 2024.05 | TPAMI-24 | [GitHub](https://github.com/expectorlin/CONSOLE) |
| [**InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment**](https://arxiv.org/pdf/2406.04882) | 2024.06 | CoRL-24 | [GitHub](https://github.com/LYX0501/InstructNav) |
| [**MAGIC: Meta-Ability Guided Interactive Chain-of-Distillation for Effective-and-Efficient Vision-and-Language Navigation**](https://arxiv.org/pdf/2406.17960) | 2024.06 | ArXiv-24 | [GitHub](https://github.com/CrystalSixone/VLN-MAGIC) |
| [**Navi2Gaze: Leveraging Foundation Models for Navigation and Target Gazing**](https://arxiv.org/pdf/2407.09053) | 2024.07 | ArXiv-24 | [GitHub](https://github.com/zhujun3753/Navi2Gaze) |
| [**TrustNavGPT: Modeling Uncertainty to Improve Trustworthiness of Audio-Guided LLM-Based Robot Navigation**](https://arxiv.org/pdf/2402.10670) | 2024.08 | IROS-24 |  |
| [**Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions**](https://arxiv.org/pdf/2408.04168) | 2024.08 | ArXiv-24 | [GitHub](https://anonymous.4open.science/r/PReP-13B5/readme.md) |
| [**Reflex-Based Open-Vocabulary Navigation without Prior Knowledge Using Omnidirectional Camera and Multiple Vision-Language Models**](https://arxiv.org/pdf/2408.11380) | 2024.08 | AR-24 |  |
| [**NOLO: Navigate Only Look Once**](https://arxiv.org/pdf/2408.01384) | 2024.08 | AR-24 |  |
| [**BehAV: Behavioral Rule Guided Autonomy Using VLMs for Robot Navigation in Outdoor Scenes**](https://arxiv.org/pdf/2409.16484) | 2024.09 | ArXiv-24 | [GitHub](https://github.com/GAMMA-UMD-Outdoor-Navigation/BehAV) |
| [**ReMEmbR: Building and Reasoning Over Long-Horizon Spatio-Temporal Memory for Robot Navigation**](https://arxiv.org/pdf/2409.13682) | 2024.09 | ArXiv-24 | [GitHub](https://github.com/NVIDIA-AI-IOT/remembr) |
| [**Tag Map: A Text-Based Map for Spatial Reasoning and Navigation with Large Language Models**](https://arxiv.org/pdf/2409.15451) | 2024.09 | CoRL-24 |  |
| [**Find Everything: A General Vision Language Model Approach to Multi-Object Search**](https://arxiv.org/pdf/2410.00388) | 2024.10 | ICRA-25 | [GitHub](https://find-all-my-things.github.io/) |
| [**DivScene: Benchmarking LVLMs for Object Navigation with Diverse Scenes and Objects**](https://arxiv.org/pdf/2410.02730) | 2024.10 | ArXiv-24 | [GitHub](https://github.com/zhaowei-wang-nlp/DivScene) |
| [**ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination**](https://openreview.net/pdf?id=vQFw9ryKyK) | 2024.10 | Submit to ICLR-25 |  |



## Navigation-OV




