# Embodied Navigation

---
<font size=5><center><b> Contents </b> </center></font>
- [Review](#review)
- [Navigation-LLM](#navigation-llm)
- [Navigation-OV](#navigation-ov)
- [Navigation-AV](#navigation-av)
---


## Review
|  Title  |   Date  |   Venue   |   Code   |
|:--------|:--------:|:--------:|:--------:|
| [**Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions**](https://arxiv.org/pdf/2203.12667) | 2022.03 | ACL-22 | [GitHub](https://github.com/eric-ai-lab/awesome-vision-language-navigation) |
| [**Advances in Embodied Navigation Using Large Language Models: A Survey**](https://arxiv.org/pdf/2311.00530) | 2023.11 | EAAI-24 | [GitHub](https://github.com/Rongtao-Xu/Awesome-LLM-EN) |
| [**A Survey of Object Goal Navigation**](https://ieeexplore.ieee.org/document/10475904) | 2024.03 | TASE-24 |  |
| [**Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models**](https://arxiv.org/pdf/2407.07035) | 2024.07 | ArXiv-24 |  |
| [**Recent Advances in Robot Navigation via Large Language Models: A Review**](https://www.researchgate.net/publication/384537380_Recent_Advances_in_Robot_Navigation_via_Large_Language_Models_A_Review) | 2024.10 | ArXiv-24 |  |
| [**Embodied navigation with multi-modal information: A survey from tasks to methodology**](https://arxiv.org/pdf/2203.12667) | 2024.12 | IF-24 | [GitHub](https://github.com/eric-ai-lab/awesome-vision-language-navigation) |



## Navigation-LLM
|  Title  |   Date  |   Venue   |   Code   |
|:--------|:--------:|:--------:|:--------:|
| [**LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action**](https://arxiv.org/pdf/2207.04429) | 2022.07 | PMLR-23 | [GitHub](https://github.com/blazejosinski/lm_nav) |
| [**Visual Language Maps for Robot Navigation**](https://arxiv.org/pdf/2210.05714) | 2022.10 | ICRA-23 | [GitHub](https://github.com/vlmaps/vlmaps) |
| [**SQA3D: Situated Question Answering in 3D Scenes**](https://arxiv.org/pdf/2210.07474) | 2022.10 | ICLR-23 | [GitHub](https://github.com/SilongYong/SQA3D) |
| [**CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation**](https://arxiv.org/pdf/2211.16649) | 2022.11 | CoRL-W-22 |  |
| [**ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation**](https://arxiv.org/pdf/2301.13166) | 2023.01 | ICML-23 |  |
| [**Can an Embodied Agent Find Your "Cat-shaped Mug"? LLM-Guided Exploration for Zero-Shot Object Navigation**](https://arxiv.org/pdf/2303.03480) | 2023.03 | IRAL-24 | [GitHub](https://github.com/vdorbala/LGX) |
| [**L3MVN: Leveraging Large Language Models for Visual Target Navigation**](https://arxiv.org/pdf/2304.05501) | 2023.04 | IROS-23 | [GitHub](https://github.com/ybgdgh/L3MVN) |
| [**NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models**](https://arxiv.org/pdf/2305.16986) | 2023.05 | AAAI-24 | [GitHub](https://github.com/GengzeZhou/NavGPT) |
| [**CorNav: Autonomous Agent with Self-Corrected Planning for Zero-Shot Vision-and-Language Navigation**](https://arxiv.org/pdf/2306.10322) | 2023.06 | ArXiv-23 | [GitHub](https://github.com/mligg23/CorNav-Site) |
| [**VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View**](https://arxiv.org/pdf/2307.06082) | 2023.07 | AAAI-24 | [GitHub](https://github.com/raphael-sch/VELMA) |
| [**A2 Nav: Action-Aware Zero-Shot Robot Navigation by Exploiting Vision-and-Language Ability of Foundation Models**](https://arxiv.org/pdf/2308.07997) | 2023.08 | NeurIPS-W-23 |  |
| [**March in Chat: Interactive Prompting for Remote Embodied Referring Expression**](https://arxiv.org/pdf/2308.10141) | 2023.08 | ICCV-23 | [GitHub](https://github.com/YanyuanQiao/MiC) |
| [**Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill**](https://arxiv.org/pdf/2309.10309) | 2023.09 | ICRA-24 | [GitHub](https://github.com/wzcai99/Pixel-Navigator) |
| [**Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions**](https://arxiv.org/pdf/2309.11382) | 2023.09 | ICRA-24 | [GitHub](https://github.com/LYX0501/DiscussNav) |
| [**Optimal Scene Graph Planning with Large Language Model Guidance**](https://arxiv.org/pdf/2309.09182) | 2023.09 | ICRA-24 | [GitHub](https://github.com/ExistentialRobotics/LLM-Scene-Graph-LTL-Planning) |
| [**SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments**](https://arxiv.org/pdf/2309.04077) | 2023.09 | ICAPS-24 | [GitHub](https://github.com/arajv/SayNav) |
| [**Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation**](https://arxiv.org/pdf/2309.08138) | 2023.09 | NeurIPS-23 | [GitHub](https://github.com/whcpumpkin/Demand-driven-navigation) |
| [**Reasoning about the Unseen for Efficient Outdoor Object Navigation**](https://arxiv.org/pdf/2309.10103) | 2023.09 | ArXiv-23 | [GitHub](https://github.com/quantingxie/ReasonedExplorer) |
| [**Multimodal Large Language Model for Visual Navigation**](https://arxiv.org/pdf/2310.08669) | 2023.10 | ArXiv-23 |  |
| [**LangNav: Language as a Perceptual Representation for Navigation**](https://arxiv.org/pdf/2310.07889) | 2023.10 | ArXiv-23 |  |
| [**Vision and Language Navigation in the Real World via Online Visual Language Mapping**](https://arxiv.org/pdf/2310.10822) | 2023.10 | ArXiv-23 |  |
| [**Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning**](https://arxiv.org/pdf/2310.10103) | 2023.10 | CoRL-23 | [GitHub](https://github.com/Michael-Equi/lfg-nav) |
| [**ThinkBot: Embodied Instruction Following with Thought Chain Reasoning**](https://arxiv.org/pdf/2312.07062) | 2023.12 | Submit to ICLR-25 |  |
| [**Towards Learning a Generalist Model for Embodied Navigation**](https://arxiv.org/pdf/2312.02010) | 2023.12 | CVPR-24 | [GitHub](https://github.com/zd11024/NaviLLM) |
| [**VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model**](https://arxiv.org/pdf/2401.02695) | 2024.01 | ICML-24 |  |
| [**MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation**](https://arxiv.org/pdf/2401.07314) | 2024.01 | ACL-24 | [GitHub](https://github.com/chen-judge/MapGPT/) |
| [**OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models**](https://arxiv.org/pdf/2402.10670) | 2024.02 | NAACL-24 | [GitHub](https://github.com/yxKryptonite/OpenFMNav) |
| [**Verifiably Following Complex Robot Instructions with Foundation Models**](https://arxiv.org/pdf/2402.11498) | 2024.02 | ArXiv-24 |  |
| [**DOZE: A Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments**](https://arxiv.org/pdf/2402.19007) | 2024.02 | IRAL-24 | [GitHub](https://github.com/JiMa25/DOZE-Dataset) |
| [**TriHelper: Zero-Shot Object Navigation with Dynamic Assistance**](https://arxiv.org/pdf/2403.15223) | 2024.03 | IROS-24 |  |
| [**Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation**](https://arxiv.org/pdf/2403.14163) | 2024.03 | Submit to AEI-24 | [GitHub](https://github.com/yxKryptonite/OpenFMNav) |
| [**Correctable Landmark Discovery via Large Models for Vision-Language Navigation**](https://arxiv.org/pdf/2405.18721) | 2024.05 | TPAMI-24 | [GitHub](https://github.com/expectorlin/CONSOLE) |
| [**InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment**](https://arxiv.org/pdf/2406.04882) | 2024.06 | CoRL-24 | [GitHub](https://github.com/LYX0501/InstructNav) |
| [**MAGIC: Meta-Ability Guided Interactive Chain-of-Distillation for Effective-and-Efficient Vision-and-Language Navigation**](https://arxiv.org/pdf/2406.17960) | 2024.06 | ArXiv-24 | [GitHub](https://github.com/CrystalSixone/VLN-MAGIC) |
| [**Navi2Gaze: Leveraging Foundation Models for Navigation and Target Gazing**](https://arxiv.org/pdf/2407.09053) | 2024.07 | ArXiv-24 | [GitHub](https://github.com/zhujun3753/Navi2Gaze) |
| [**TrustNavGPT: Modeling Uncertainty to Improve Trustworthiness of Audio-Guided LLM-Based Robot Navigation**](https://arxiv.org/pdf/2402.10670) | 2024.08 | IROS-24 |  |
| [**Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions**](https://arxiv.org/pdf/2408.04168) | 2024.08 | ArXiv-24 | [GitHub](https://anonymous.4open.science/r/PReP-13B5/readme.md) |
| [**Reflex-Based Open-Vocabulary Navigation without Prior Knowledge Using Omnidirectional Camera and Multiple Vision-Language Models**](https://arxiv.org/pdf/2408.11380) | 2024.08 | AR-24 |  |
| [**NOLO: Navigate Only Look Once**](https://arxiv.org/pdf/2408.01384) | 2024.08 | AR-24 |  |
| [**BehAV: Behavioral Rule Guided Autonomy Using VLMs for Robot Navigation in Outdoor Scenes**](https://arxiv.org/pdf/2409.16484) | 2024.09 | ArXiv-24 | [GitHub](https://github.com/GAMMA-UMD-Outdoor-Navigation/BehAV) |
| [**ReMEmbR: Building and Reasoning Over Long-Horizon Spatio-Temporal Memory for Robot Navigation**](https://arxiv.org/pdf/2409.13682) | 2024.09 | ArXiv-24 | [GitHub](https://github.com/NVIDIA-AI-IOT/remembr) |
| [**Tag Map: A Text-Based Map for Spatial Reasoning and Navigation with Large Language Models**](https://arxiv.org/pdf/2409.15451) | 2024.09 | CoRL-24 |  |
| [**Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation in Continuous Environment with Open-Source LLMs**](https://arxiv.org/pdf/2409.18794) | 2024.09 | ArXiv-24 |  |
| [**Find Everything: A General Vision Language Model Approach to Multi-Object Search**](https://arxiv.org/pdf/2410.00388) | 2024.10 | ICRA-25 | [GitHub](https://find-all-my-things.github.io/) |
| [**DivScene: Benchmarking LVLMs for Object Navigation with Diverse Scenes and Objects**](https://arxiv.org/pdf/2410.02730) | 2024.10 | ArXiv-24 | [GitHub](https://github.com/zhaowei-wang-nlp/DivScene) |
| [**ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination**](https://openreview.net/pdf?id=vQFw9ryKyK) | 2024.10 | Submit to ICLR-25 |  |


## Navigation-OV
|  Title  |   Date  |   Venue   |   Code   |
|:--------|:--------:|:--------:|:--------:|
| [**Simple but Effective: CLIP Embeddings for Embodied AI**](https://arxiv.org/pdf/2111.09888) | 2021.11 | CVPR-22 | [GitHub](https://github.com/allenai/embodied-clip) |
| [**Zero Experience Required: Plug & Play Modular Transfer Learning for Semantic Visual Navigation**](https://arxiv.org/pdf/2202.02440) | 2022.02 | CVPR-22 | [GitHub](https://github.com/ziadalh/zero_experience_required) |
| [**CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation**](https://arxiv.org/pdf/2203.10421) | 2022.03 | CVPR-23 | [GitHub](https://github.com/real-stanford/cow) |
| [**ZSON: Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings**](https://arxiv.org/pdf/2206.12403) | 2022.06 | NeurIPS-22 | [GitHub](https://github.com/gunagg/zson) |
| [**Zero-Shot Object Searching Using Large-scale Object Relationship Prior**](https://arxiv.org/pdf/2303.06228) | 2023.03 | ArXiv-23 |  |
| [**Semantic Policy Network for Zero-Shot Object Goal Visual Navigation**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10265178) | 2023.09 | IRAL-23 |  |
| [**Active Open-Vocabulary Recognition: Let Intelligent Moving Mitigate CLIP Limitations**](https://arxiv.org/pdf/2311.17938) | 2023.11 | CVPR-24 |  |
| [**Prioritized Semantic Learning for Zero-shot Instance Navigation**](https://arxiv.org/pdf/2403.11650) | 2024.03 | ECCV-24 | [GitHub](https://github.com/XinyuSun/PSL-InstanceNav) |
| [**OVExp: Open Vocabulary Exploration for Object-Oriented Navigation**](https://arxiv.org/pdf/2407.09016) | 2024.07 | ArXiv-24 | [GitHub](https://github.com/OpenRobotLab/OVExp) |


## Navigation-AV
|  Title  |   Date  |   Venue   |   Code   |
|:--------|:--------:|:--------:|:--------:|
| [**SoundSpaces: Audio-Visual Navigation in 3D Environments**](https://arxiv.org/pdf/1912.11474) | 2019.12 | ECCV-20 | [GitHub](https://github.com/facebookresearch/sound-spaces) |
| [**Look, Listen, and Act: Towards Audio-Visual Embodied Navigation**](https://arxiv.org/pdf/1912.11684) | 2019.12 | ICRA-20 |  |
| [**Learning to Set Waypoints for Audio-Visual Navigation**](https://arxiv.org/pdf/2008.09622) | 2020.08 | ICLR-21 | [GitHub](https://github.com/facebookresearch/sound-spaces) |
| [**Semantic Audio-Visual Navigation**](https://arxiv.org/pdf/2012.11583) | 2020.12 | CVPR-21 | [GitHub](https://github.com/facebookresearch/sound-spaces) |
| [**Move2Hear: Active Audio-Visual Source Separation**](https://arxiv.org/pdf/2105.07142) | 2021.05 | ICCV-21 | [GitHub](https://github.com/SAGNIKMJR/move2hear-active-AV-separation) |
| [**Catch Me If You Hear Me: Audio-Visual Navigation in Complex Unmapped Environments with Moving Sounds**](https://arxiv.org/pdf/2111.14843) | 2021.11 | IRAL-23 | [GitHub](https://github.com/robot-learning-freiburg/Dav-Nav) |
| [**Active Audio-Visual Separation of Dynamic Sound Sources**](https://arxiv.org/pdf/2202.00850) | 2022.02 | ECCV-22 | [GitHub](https://github.com/SAGNIKMJR/active-av-dynamic-separation) |
| [**Sound Adversarial Audio-Visual Navigation**](https://arxiv.org/pdf/2202.10910) | 2022.02 | ICLR-22 | [GitHub](https://github.com/yyf17/SAAVN/tree/main) |
| [**Towards Generalisable Audio Representations for Audio-Visual Navigation**](https://arxiv.org/pdf/2206.00393) | 2022.06 | CVPR-W-22 | [GitHub](https://github.com/ShunqiM/AV-GeN) |
| [**Finding Fallen Objects Via Asynchronous Audio-Visual Integration**](https://arxiv.org/pdf/2207.03483) | 2022.07 | CVPR-22 | [GitHub](https://github.com/chuangg/find_fallen_objects) |
| [**Pay Self-Attention to Audio-Visual Navigation**](https://arxiv.org/pdf/2210.01353) | 2022.10 | BMVC-22 |  |
| [**AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments**](https://arxiv.org/pdf/2210.07940) | 2022.10 | NeurIPS-22 | [GitHub](https://github.com/merlresearch/avlen) |
| [**Towards Versatile Embodied Navigation**](https://arxiv.org/pdf/2210.16822) | 2022.10 | NeurIPS-22 | [GitHub](https://github.com/hanqingwangai/VXN) |
| [**Knowledge-driven Scene Priors for Semantic Audio-Visual Embodied Navigation**](https://arxiv.org/pdf/2212.11345) | 2022.12 | ArXiv-22 | [GitHub](https://github.com/SAGNIKMJR/active-av-dynamic-separation) |
| [**Learning Semantic-Agnostic and Spatial-Aware Representation for Generalizable Visual-Audio Navigation**](https://arxiv.org/pdf/2304.10773) | 2023.04 | IRAL-23 |  |
| [**CAVEN: An Embodied Conversational Agent for Efficient Audio-Visual Navigation in Noisy Environments**](https://arxiv.org/pdf/2306.04047) | 2023.06 | AAAI-24 |  |
| [**Multi-goal Audio-visual Navigation using Sound Direction Map**](https://arxiv.org/pdf/2308.00219) | 2023.08 | IROS-23 |  |
| [**Sim2Real Transfer for Audio-Visual Navigation with Frequency-Adaptive Acoustic Field Prediction**](https://arxiv.org/pdf/2405.02821) | 2024.05 | IROS-24 |  |
| [**RILA: Reflective and Imaginative Language Agent for Zero-Shot Semantic Audio-Visual Navigation**](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_RILA_Reflective_and_Imaginative_Language_Agent_for_Zero-Shot_Semantic_Audio-Visual_CVPR_2024_paper.pdf) | 2024.06 | CVPR-24 |  |



